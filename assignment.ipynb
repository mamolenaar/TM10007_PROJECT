{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CiDn2Sk-VWqE",
        "outputId": "1977955b-ab7e-424d-d179-7ca2e4a4af77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/mamolenaar/TM10007_PROJECT_SvdMeijden_MMolenaar.git      #SIRI: mamolenaar moet je even aanpassen"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for brats (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ZsvmQBVuXM",
        "colab_type": "text"
      },
      "source": [
        "##Taakverdeling eindopdracht:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "1.   **Datapreperatie**\n",
        "\n",
        "\n",
        "* *Verwijderen duplicaten kolommen (als aanwezig)*   DONE \n",
        "*  *Nulwaarden verwijderen* (Siri)        DONE\n",
        "* *Trainingset/validatieset* (Mitch)       DONE\n",
        "* Testing outliers with Z-score (Mitch)\n",
        "*   Scaling features (Mitch)\n",
        "* PCA?\n",
        "* Feature selection \n",
        "\n",
        "\n",
        "2.   **Random Forest**\n",
        "* Bagging/bootstrap aggregating \n",
        "* Hyperparameter tuning\n",
        "* Ensembling \n",
        "3. **Support Vector Machine**\n",
        "* Kernel\n",
        "* Hyperparameter tuning \n",
        "4. **Multilayer neural network** \n",
        "* Activation and loss function\n",
        "5. **Convolutional NN/Recurrent NN** \n",
        "(Ik weet niet zeker of we deze ook moeten toepassen) \n",
        "\n",
        "\n",
        "Checklist punten verslag / punten om overna te denken\n",
        "1. Datapreparatie\n",
        "- Hoeveel nulwaarden/NaNs zijn er? in welke kolommen komen deze voor? \n",
        "  komen er veel NaNs voor in dezelfde kolom? Zo ja, is het geoorloofd om deze kolommen te imputeren? Op welke manier imputeren (op basis van andere features (multivariate imputation) of eigen features: zie https://scikit-learn.org/stable/modules/impute.html\n",
        "\n",
        "- Imputatie: https://towardsdatascience.com/handling-missing-data-for-a-beginner-6d6f5ea53436\n",
        "\n",
        "- Hebben we veel kolommen waarin outliers zitten? Zo ja, dan moeten we hierop scaling aanpassen, zie: https://scikit-learn.org/stable/modules/preprocessing.html. Testen outliers met Z-score:https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba\n",
        "\n",
        "- We nemen aan dat basic classifiers (KNN, linear, quadratic etc.) te simpel zijn om te gebruiken (toch?). Wat kan het doen van feature selection hierover zeggen?\n",
        "Als we toch basic classifiers gebruiken, moeten we hier voorafgaand PCA op toepassen?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLL0JMQodTOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######## Import packages and models ##########################################\n",
        "\n",
        "# Import general packages\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets as ds\n",
        "import seaborn\n",
        "from sklearn import model_selection\n",
        "\n",
        "\n",
        "# import classifiers\n",
        "from sklearn import model_selection\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# import models\n",
        "models = [\n",
        "    ('NB', GaussianNB()),\n",
        "    ('LDA', LinearDiscriminantAnalysis()),\n",
        "    ('QDA', QuadraticDiscriminantAnalysis()),\n",
        "    ('LR', LogisticRegression()),\n",
        "    ('SGD', SGDClassifier()),\n",
        "    ('DT', DecisionTreeClassifier()),\n",
        "    ('KNN', KNeighborsClassifier()),   #hieronder verdere modellen toevoegen\n",
        "]\n",
        "\n",
        "# Import datapreprocessing\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Import split data in train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score # allows one scoring metric\n",
        "from sklearn.model_selection import cross_validate # allows us to use multiple scoring metrics\n",
        "\n",
        "# Import metrics\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-NE_fTbKGe5z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "b459e95b-e9d2-42ee-e50a-f5a550dcc543"
      },
      "source": [
        "# Data loading functions. \n",
        "from hn.load_data import load_data\n",
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n",
        "data.info\n",
        "# Find labels assigned for tumor classification \n",
        "def unique(list1): \n",
        "    x = np.array(list1) \n",
        "    print(np.unique(x)) \n",
        "\n",
        "print(\"The unique labels assigned to each patient for tumor classification are\") \n",
        "unique(data['label'])\n"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of samples: 113\n",
            "The number of columns: 160\n",
            "The unique labels assigned to each patient for tumor classification are\n",
            "['T12' 'T34']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdOFTFiBduX3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "a936e267-5ec6-4ff9-b460-b3350ae50b99"
      },
      "source": [
        "########################## Data preperation ##########################\n",
        "\n",
        "# Replace zero values with NaN \n",
        "data = data.replace(0, np.NaN)\n",
        "# count the number of NaN values in each column\n",
        "print(data.isnull().sum())\n",
        "\n",
        "data = data.loc[:,~data.columns.duplicated()]   # delete duplicated columns if available\n",
        "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean') # impute NaN values with mean values\n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hf_energy               0\n",
            "hf_entropy              0\n",
            "hf_kurtosis             0\n",
            "hf_max                  0\n",
            "hf_mean                 0\n",
            "                       ..\n",
            "tf_NGTDM_Busyness      54\n",
            "tf_NGTDM_Coarseness     0\n",
            "tf_NGTDM_Complexity    54\n",
            "tf_NGTDM_Contrast      54\n",
            "tf_NGTDM_Strength      54\n",
            "Length: 160, dtype: int64\n",
            "               hf_energy  hf_entropy  ...  tf_NGTDM_Contrast  tf_NGTDM_Strength\n",
            "ID                                    ...                                      \n",
            "0_HN1006_0  24345.357124    3.444203  ...                NaN                NaN\n",
            "0_HN1022_0  35301.370880    2.873434  ...       1.090935e-04           0.018735\n",
            "0_HN1026_0   6340.950214    2.769541  ...       3.528037e-06           0.672940\n",
            "0_HN1029_0   5690.500179    3.073200  ...       1.917977e-04           0.062940\n",
            "0_HN1046_0  15553.548185    3.025631  ...       2.093031e-04           0.030421\n",
            "...                  ...         ...  ...                ...                ...\n",
            "0_HN1950_0  50004.678306    2.246432  ...                NaN                NaN\n",
            "0_HN1954_0  58235.863884    2.554536  ...       1.381761e-06           0.137259\n",
            "0_HN1968_0  16630.150022    2.341213  ...       9.970363e-08           1.423791\n",
            "0_HN1987_0  58087.164347    2.159815  ...       5.282298e-05           0.013049\n",
            "0_HN1998_0  12724.205929    3.085224  ...       6.309797e-06           0.206874\n",
            "\n",
            "[113 rows x 160 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEnRIt6q2Mbv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "4bd2c160-711e-4cea-c043-4c899e595c3c"
      },
      "source": [
        "##########################Comparing models ###########################\n",
        "## Hieronder eerst splitsing in train (validatie en train) en test set. V\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # split train and test\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) # split train in validation and train\n",
        "\n",
        "# Hieronder toepassen van de verschillende modellen op de trainingsdata. Vervolgens berekenen metrics op validatie set. \n",
        "# Bereking metrics op testset is nog niet gebeurt. Hiervoor moet nog optimalisatie van parameters plaatsvinden (hoeveel splitsingen zijn nodig etc.)\n",
        "dic= {}\n",
        "scoring = ['roc_auc', 'accuracy', 'f1_macro', 'precision_macro', 'recall_macro']\n",
        "for name, model in models:\n",
        "        clf = model\n",
        "        clf.fit(X_train, y_train)\n",
        "        accuracy = clf.score(X_val, y_val)        # accuracy tested on validation data without cross validation\n",
        "        #accuracy_CV = cross_val_score(clf, X, y, cv=5)\n",
        "        for metric in scoring:                      # cross validation\n",
        "            accuracy_CV = cross_validate(clf, X_val, y_val, cv=5, scoring=metric, return_train_score=False) # cv can also return train score but we set it to false\n",
        "            test_score = accuracy_CV[\"test_score\"]\n",
        "            #dic = {name: test_score}\n",
        "            dic[name, metric]=pd.DataFrame({'mean_score': {0:test_score.mean()}, 'std_score': {0:test_score.std()*2}})\n",
        "##########################Cross- validation###########################"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-68b2a360f087>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# accuracy tested on validation data without cross validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#accuracy_CV = cross_val_score(clf, X, y, cv=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m--> 208\u001b[0;31m                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ]
    }
  ]
}