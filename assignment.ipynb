{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CiDn2Sk-VWqE",
        "outputId": "43c59fcd-35fa-451f-d6cd-c52cb9472adf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/mamolenaar/TM10007_PROJECT_SvdMeijden_MMolenaar.git      #SIRI: mamolenaar moet je even aanpassen"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for brats (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ZsvmQBVuXM",
        "colab_type": "text"
      },
      "source": [
        "##Taakverdeling eindopdracht:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "1.   **Datapreperatie**\n",
        "\n",
        "\n",
        "* *Verwijderen duplicaten kolommen (als aanwezig)*   DONE \n",
        "*  *Nulwaarden verwijderen* (Siri)        DONE\n",
        "* *Trainingset/validatieset* (Mitch)       DONE\n",
        "* Testing outliers with Z-score (Mitch)\n",
        "*   Scaling features (Mitch)\n",
        "* PCA?\n",
        "* Feature selection \n",
        "\n",
        "\n",
        "2.   **Random Forest**\n",
        "* Bagging/bootstrap aggregating \n",
        "* Hyperparameter tuning\n",
        "* Ensembling \n",
        "3. **Support Vector Machine**\n",
        "* Kernel\n",
        "* Hyperparameter tuning \n",
        "4. **Multilayer neural network** \n",
        "* Activation and loss function\n",
        "5. **Convolutional NN/Recurrent NN** \n",
        "(Ik weet niet zeker of we deze ook moeten toepassen) \n",
        "\n",
        "\n",
        "Checklist punten verslag / punten om overna te denken\n",
        "1. Datapreparatie\n",
        "- de data is numeriek (geen categorical data).\n",
        "- De samples met de meest missende feature waardes: HN1106 (16 missing values), HN1600 (14 missing values), HN1760 (17 missing values). De rest van de data heeft 0, 3 of 7 features die 0 zijn. \n",
        "- De features met de meest missende sample waardes: tff_GLRLM_GrayLevelVariance (54 missing features), tff_GLSZM_GrayLevelVariance (54 missing features), tf_GLSZM_ZoneVariane (53), Tf_NGTDM_Busyness (54),  Tf_NGTDM_complexity (54),  Tf_NGTDM_contrast (54),  Tf_NGTDM_Strength (54). Veel missende waardes komen voor onder verschillende features. De andere features hebbben 0, 3 of 5 missende waarden.\n",
        "\n",
        "Keuze: Omdat we relatief weinig samples hebben (113) is het niet handig om de samples weg te halen. Het lijkt mij handig om de features die veel samples missen te verwijderen. Verder is het onduidelijk of we zomaar alle nullen kunnen imputeren. Sommige features kunnen bijvoorbeeld echt de waarde 0 hebben (misschien moeten we dit zien als discussiepunt).\n",
        "\n",
        "Op welke manier imputeren (op basis van andere features (multivariate imputation) of eigen features: zie https://scikit-learn.org/stable/modules/impute.html\n",
        "\n",
        "- Imputatie: https://towardsdatascience.com/handling-missing-data-for-a-beginner-6d6f5ea53436, https://scikit-learn.org/stable/modules/impute.html\n",
        "\n",
        "\n",
        "- Hebben we veel kolommen waarin outliers zitten? Zo ja, dan moeten we hierop scaling aanpassen, zie: https://scikit-learn.org/stable/modules/preprocessing.html. Testen outliers met Z-score:https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba\n",
        "\n",
        "- We nemen aan dat basic classifiers (KNN, linear, quadratic etc.) te simpel zijn om te gebruiken (toch?). Wat kan het doen van feature selection hierover zeggen?\n",
        "Als we toch basic classifiers gebruiken, moeten we hier voorafgaand PCA op toepassen?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLL0JMQodTOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######## Import packages and models ##########################################\n",
        "\n",
        "# Import general packages\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets as ds\n",
        "import seaborn\n",
        "from sklearn import model_selection\n",
        "\n",
        "\n",
        "# import classifiers\n",
        "from sklearn import model_selection\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# import models\n",
        "models = [\n",
        "    ('NB', GaussianNB()),\n",
        "    ('LDA', LinearDiscriminantAnalysis()),\n",
        "    ('QDA', QuadraticDiscriminantAnalysis()),\n",
        "    ('LR', LogisticRegression()),\n",
        "    ('SGD', SGDClassifier()),\n",
        "    ('DT', DecisionTreeClassifier()),\n",
        "    ('KNN', KNeighborsClassifier()),   #hieronder verdere modellen toevoegen\n",
        "]\n",
        "\n",
        "# Import datapreprocessing\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Import split data in train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score # allows one scoring metric\n",
        "from sklearn.model_selection import cross_validate # allows us to use multiple scoring metrics\n",
        "\n",
        "# Import metrics\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-NE_fTbKGe5z",
        "outputId": "13ed128b-58a0-48d8-8ec5-0fc1bba7aedf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Data loading functions. \n",
        "from hn.load_data import load_data\n",
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n",
        "data.info\n",
        "# Find labels assigned for tumor classification \n",
        "def unique(list1): \n",
        "    x = np.array(list1) \n",
        "    print(np.unique(x)) \n",
        "\n",
        "print(\"The unique labels assigned to each patient for tumor classification are \") \n",
        "unique(data['label'])\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of samples: 113\n",
            "The number of columns: 160\n",
            "The unique labels assigned to each patient for tumor classification are \n",
            "['T12' 'T34']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5EVAPG-H3Mb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "abdf47b3-1781-4555-8d28-eff5de97d719"
      },
      "source": [
        "(data == 0).astype(bool).sum(axis=1)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID\n",
              "0_HN1006_0    7\n",
              "0_HN1022_0    0\n",
              "0_HN1026_0    0\n",
              "0_HN1029_0    0\n",
              "0_HN1046_0    0\n",
              "             ..\n",
              "0_HN1950_0    7\n",
              "0_HN1954_0    0\n",
              "0_HN1968_0    0\n",
              "0_HN1987_0    0\n",
              "0_HN1998_0    0\n",
              "Length: 113, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdOFTFiBduX3",
        "colab_type": "code",
        "outputId": "c00bdcfb-9e7c-4e90-fb16-7fdb4c1002bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "########################## Data preperation ##########################\n",
        "# select labels\n",
        "y= data['label']\n",
        "# select data without labels\n",
        "X = data.drop(['label'], axis=1)\n",
        "# Splitting data sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # split train and test so that we don't touch test set.\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) # split train in validation and train\n",
        "\n",
        "# Remove features with >53 missing values.\n",
        "X_train = (X_train.drop(['tf_GLRLM_GrayLevelVariance', 'tf_GLSZM_GrayLevelVariance', 'tf_GLSZM_ZoneVariance', 'tf_NGTDM_Busyness', 'tf_NGTDM_Complexity', 'tf_NGTDM_Contrast', 'tf_NGTDM_Strength'], axis=1))\n",
        "\n",
        "\n",
        "# Replace zero values with NaN \n",
        "data = data.replace(0, np.NaN)\n",
        "# count the number of NaN values in each column\n",
        "print(data.isnull().sum())\n",
        "\n",
        "data = data.loc[:,~data.columns.duplicated()]   # delete duplicated columns if available\n",
        "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean') # impute NaN values with mean values\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hf_energy               0\n",
            "hf_entropy              0\n",
            "hf_kurtosis             0\n",
            "hf_max                  0\n",
            "hf_mean                 0\n",
            "                       ..\n",
            "tf_NGTDM_Busyness      54\n",
            "tf_NGTDM_Coarseness     0\n",
            "tf_NGTDM_Complexity    54\n",
            "tf_NGTDM_Contrast      54\n",
            "tf_NGTDM_Strength      54\n",
            "Length: 160, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEnRIt6q2Mbv",
        "colab_type": "code",
        "outputId": "13fcb75b-9448-400c-8794-2c38b4430922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "##########################Comparing models ###########################\n",
        "## Hieronder eerst splitsing in train (validatie en train) en test set. V\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # split train and test\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) # split train in validation and train\n",
        "\n",
        "# Hieronder toepassen van de verschillende modellen op de trainingsdata. Vervolgens berekenen metrics op validatie set. \n",
        "# Bereking metrics op testset is nog niet gebeurt. Hiervoor moet nog optimalisatie van parameters plaatsvinden (hoeveel splitsingen zijn nodig etc.)\n",
        "dic= {}\n",
        "scoring = ['roc_auc', 'accuracy', 'f1_macro', 'precision_macro', 'recall_macro']\n",
        "for name, model in models:\n",
        "        clf = model\n",
        "        clf.fit(X_train, y_train)\n",
        "        accuracy = clf.score(X_val, y_val)        # accuracy tested on validation data without cross validation\n",
        "        #accuracy_CV = cross_val_score(clf, X, y, cv=5)\n",
        "        for metric in scoring:                      # cross validation\n",
        "            accuracy_CV = cross_validate(clf, X_val, y_val, cv=5, scoring=metric, return_train_score=False) # cv can also return train score but we set it to false\n",
        "            test_score = accuracy_CV[\"test_score\"]\n",
        "            #dic = {name: test_score}\n",
        "            dic[name, metric]=pd.DataFrame({'mean_score': {0:test_score.mean()}, 'std_score': {0:test_score.std()*2}})\n",
        "##########################Cross- validation###########################"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:691: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}